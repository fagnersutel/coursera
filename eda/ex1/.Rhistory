#contra a vari??vel de desfecho, atrav??s da linha de melhor ajuste, como visto abaixo.
par(mfrow=c(1,1))#divide a area de plotagem em uma coluna
scatter.smooth(x=cars$speed, y=cars$dist, mail="Dist ~Speed")#scatterplot
par(mfrow=c(1,2))#divide a area de plotagem em dias colunas
boxplot(cars$speed, main="Speed", sub=paste("Outlier rows: ", boxplot.stats(cars$speed)$out))  #plotagem para 'Speed'
boxplot(cars$dist, main="Distances", sub=paste("Outlier rows: ", boxplot.stats(cars$dist)$out))  #plotagem para 'Speed'
#DENSITY PLOT - VERIFICAR SE A VARI??VEL DE DESFECHO EST?? NORMALISADA
library(e1071)
par(mfrow=c(1, 2))  # divide graph area in 2 columns
plot(density(cars$speed), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$speed), 2)))  # density plot for 'speed'
polygon(density(cars$speed), col="red")
plot(density(cars$dist), main="Density Plot: Distance", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$dist), 2)))  # density plot for 'dist'
polygon(density(cars$dist), col="red")
#CORRELA????O
#Correla????o ?? uma medida estat??stica que indica o nivel de dependencia linear entre duas vari??veis que ocorrem paralelamente, no nosso caso temos a rela????o speed dist.
#A correla????o obtem valores ente -1 e +1, se nos observarmos para todas instancias onde a velocidade aumenta, a distancia tamb??m aumenta paralelamente,
#isso indica uma correla????o positiva entre elas, esta observa????o indicara uma correla????o pr??xima de +1. O contrario ?? verdadeiro quando a rela????o ?? inversa, nessa caso a correla????o ?? pr??xima de -1, onde o incremento de uma vari??vel repercute do decremento da outra.
#Valores pr??ximos de zero sugerem uma rela????o fraca entre as vari??veis. Uma baixa correla????o cmo (-0.2 < X < 0.2) provavelmente sugere
#muito da varia????o d desfecho de Y ?? inesplicado por X, nesse caso deve-se buscar por polhores vari??veis explicativas.
cor(cars$speed, cars$dist) #calcula a correla????o entre velocidade e distancia
#CONSTRUINDO UM MODELO LINEAR
#Agora nos veremos uma rela????o linear visualmente no scatter plot e por meio do computo da correla????o.
#Vejamos a sintaxe para construir um modelo linear. A fun????o utilizada para modelos lineares ?? lm(). A fun????o lm() toma dois principais argumentos
#1-formila e 2-dados. Os dados s??o normalmente um data.frame e a formula ?? um objeto da classe 'formula'. Mas a conven????o mais comum pe escrever
#a formula diretamente no lugar do argumento 2, como escrito abaixo:
linearMod <- lm(dist ~ speed, data=cars)  #constroi um modelo de regressao linear para todos dados
print(linearMod)
#DIAGN??STICO DE REGRESS??O LINEAR
#Agora que construimos o modelo de regress??o e obtvemos a formula para predizer o valor de desfecho "dist" quando a velocidade "speed" ?? conhecida
#Isto ?? suficiente para realmente isar este modelo? Agora! Antes de usar o modelo de regress??o, voce deve estar seguro de que isto ?? estatisticamente significante.
#O quanto seguro ?? isso? Vamos iniciar exibindo um resumo das estatisticas para nossos dados de linearMod
summary(linearMod)
#COMO CALCULAR O T-ESTATISTICO E P-VALUES?
#Quando o modelo de coeficientes e o erro padr??o ?? conhecido, a formula para calcular --stats e p-value ?? a seguinte:
#        t-stats = B - coeficiente / erro padr??o
modelSummary <- summary(linearMod)  # recebe o resumo n??o como saida mas como objeto
modelCoeffs <- modelSummary$coefficients  # recebe os coeficientes do modelo
beta.estimate <- modelCoeffs["speed", "Estimate"]  # recebe o beta estimado para speed (preditor)
std.error <- modelCoeffs["speed", "Std. Error"]   # recebe o erro padr??o de speed(preditor)
t_value <- beta.estimate/std.error  # calcula o t-statis
p_value <- 2*pt(-abs(t_value), df=nrow(cars)-ncol(cars))  # calcula p-Value
f_statistic <- linearMod$fstatistic[1]  # fstatistic
f <- summary(linearMod)$fstatistic  # parametros para o modelo p-value
model_p <- pf(f[1], f[2], f[3], lower=FALSE)
model_p
#Em [R] podemos comparar o modelo utilizamos os valors de CIA e CIB
AIC(linearMod)
BIC(linearMod)
#Passo 01: Criar os conjuntos de treino e teste a partir dos conjunto de dados original
# Create Training and Test data -
set.seed(100)  # setar o seed ??ra reprodizir os resultados em uma amostra aleat??ria
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # obtendo os indides de linhas para os dados de treino
trainingData <- cars[trainingRowIndex, ]  # dados do modelo de treino
testData  <- cars[-trainingRowIndex, ]   # dados de teste, 1-n, ou o conjunto de dados menos os dados de treino 'trainingRowIndex'
testData
#Passo 02: Desenvolver o modelo nos dados de treinoo e utilizar o mesmo para predizer a distancia nos dados de teste
# construindo o modelo sobre os dados de treino
lmMod <- lm(dist ~ speed, data=trainingData)  # constru????o do modelo
distPred <- predict(lmMod, testData)  # predi????o da distancia
distPred
#Passo 03: Revis??o das medidas de diagn??stico
summary (lmMod)  # resumo do modelo
AIC (lmMod)  # Calculo do crit??rio de informa????o akaike
actuals_preds <- data.frame(cbind(actuals=testData$dist, predicteds=distPred))  # cria um data frame com os atuais valores preditos
actuals_preds
correlation_accuracy <- cor(actuals_preds)  # retorna o valor de acur??cia
head(actuals_preds)
correlation_accuracy
#minima e maxima acuracia
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
min_max_accuracy
#media absoluta da percentagem de erro
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
mape
#Dito de outra forma elas precisam ser paralelas e t??o pr??ximas quanto poss??vel. Informa????es mais detalhadas sobre interpreta????o de gr??ficos de
# valida????o cruzada (cross valitation) n??o encontratas em artigos de constru????o de modelos lineares avan??ados.
#install.packages("DAAG")
library(DAAG)
layout(matrix(c(1,1),2,1))
# performando CV
cvResults <- suppressWarnings(CVlm(df = cars, form.lm = formula(dist ~ speed), m = 5, dots = FALSE, seed = 29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."));
cvResults <- suppressWarnings(CVlm(data = cars, form.lm = formula(dist ~ speed), m = 5, dots = FALSE, seed = 29, plotit = c("Observed","Residual"), main="Small symbols show cross-validation predicted values", legend.pos="topleft", printit = TRUE))
attr(cvResults, 'ms')
#Ao analizar o gr??fico devemos observar se:
#PARA ONDE IR AGORA?
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#modelos avan??ados de regress??o ir??o demonstrar outros casos especiais onde diferentes formas de regress??o podem ser mais adequadas.
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#modelos avan??ados de regress??o ir??o demonstrar outros casos especiais onde diferentes formas de regress??o podem ser mais adequadas.
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#modelos avan??ados de regress??o ir??o demonstrar outros casos especiais onde diferentes formas de regress??o podem ser mais adequadas.
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#modelos avan??ados de regress??o ir??o demonstrar outros casos especiais onde diferentes formas de regress??o podem ser mais adequadas.
#PARA ONDE IR AGORA?
#Cobrimos os principais conceitos sobre regress??o linear. Al??m disso, precisamos entender que a regress??o linear ?? baseada em certas suposi????es subjacentes
#que dever ser consideradas com muito cuidado, sobretudo quando trabalhamos com multiplos preditores. Uma vez familiarizado com isso, os
#modelos avan??ados de regress??o ir??o demonstrar outros casos especiais onde diferentes formas de regress??o podem ser mais adequadas.
rm(list=ls())
ls()
#Regressao linear multipla
mydata<-data.frame(x1 = c(2, 2, 6, 4), x2 = c(3, 4, 2, 8), x3 = c(3, 4, 2, 8), x4 = c(6, 5, 3, 4), y = c(4, 5, 3, 2))
mydata<-data.frame(x1 = runif(50, 10, 100), x2 = rnorm(50), x3 = rnorm(50), x4 = runif(50, 210, 420), y = rnorm(50))
#mydata <- matrix(rnorm(30), nrow=6)
fit <- lm(y ~ x1 + x2 + x3, data = mydata)
summary(fit)
# Other useful functions
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table
vcov(fit) # covariance matrix for model parameters
influence(fit) # regression diagnostics
# diagnostic plots
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit)
# compare models
fit1 <- lm(y ~ x1 + x2 + x3 + x4, data=mydata)
fit2 <- lm(y ~ x1 + x2, data=mydata)
anova(fit1, fit2)
# K-fold cross-validation
library(DAAG)
# 3 fold cross-validation
cvResults <- suppressWarnings(CVlm(data = mydata, fit, m = 3, dots = FALSE, seed = 29, plotit = c("Observed","Residual"), main="Small symbols show cross-validation predicted values", legend.pos="topleft", printit = TRUE))
fit <- lm(y~x1+x2+x3,data=mydata)
#install.packages("bootstrap")
library(bootstrap)
# define functions
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(fit,x){cbind(1,x)%*%fit$coef}
# matrix of predictors
X <- as.matrix(mydata[c("x1","x2","x3")])
# vector of predicted values
y <- as.matrix(mydata[c("y")])
results <- crossval(X,y,theta.fit,theta.predict,ngroup=10)
cor(y, fit$fitted.values)**2 # raw R2
cor(y,results$cv.fit)**2 # cross-validated R2
# Stepwise Regression
library(MASS)
fit <- lm(y~x1+x2+x3,data=mydata)
step <- stepAIC(fit, direction="both")
step$anova # display results
# All Subsets Regression
#install.packages("leaps")
library(leaps)
attach(mydata)
leaps<-regsubsets(y~x1+x2+x3+x4,data=mydata,nbest=10)
# view results
summary(leaps)
# plot a table of models showing variables in each model.
# models are ordered by the selection statistic.
plot(leaps,scale="r2")
# plot statistic by subset size
library(car)
subsets(leaps, statistic="rsq")
summary(alli.mod1)
source('~/OneDrive/Cursos/meusR/RegressaoLinear Simples.R', echo=TRUE)
alligator = data.frame(
lnLength = c(3.87, 3.61, 4.33, 3.43, 3.81, 3.83, 3.46, 3.76,
3.50, 3.58, 4.19, 3.78, 3.71, 3.73, 3.78),
lnWeight = c(4.87, 3.93, 6.46, 3.33, 4.38, 4.70, 3.50, 4.50,
3.58, 3.64, 5.90, 4.43, 4.38, 4.42, 4.25)
)
alligator
alligator = data.frame(
lnLength = c(3.87, 3.61, 4.33, 3.43, 3.81, 3.83, 3.46, 3.76,
3.50, 3.58, 4.19, 3.78, 3.71, 3.73, 3.78),
lnWeight = c(4.87, 3.93, 6.46, 3.33, 4.38, 4.70, 3.50, 4.50,
3.58, 3.64, 5.90, 4.43, 4.38, 4.42, 4.25)
)
alligator
xyplot(lnWeight ~ lnLength, data = alligator,
xlab = "Snout vent length (inches) on log scale",
ylab = "Weight (pounds) on log scale",
main = "Alligators in Central Florida"
)
alli.mod1 = lm(lnWeight ~ lnLength, data = alligator)
summary(alli.mod1)
xyplot(resid(alli.mod1) ~ fitted(alli.mod1),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residual Diagnostic Plot",
panel = function(x, y, ...)
{
panel.grid(h = -1, v = -1)
panel.abline(h = 0)
panel.xyplot(x, y, ...)
}
)
install.packages("lattice")
library(lattice)
xyplot(lnWeight ~ lnLength, data = alligator,
xlab = "Snout vent length (inches) on log scale",
ylab = "Weight (pounds) on log scale",
main = "Alligators in Central Florida"
)
alli.mod1 = lm(lnWeight ~ lnLength, data = alligator)
summary(alli.mod1)
xyplot(resid(alli.mod1) ~ fitted(alli.mod1),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residual Diagnostic Plot",
panel = function(x, y, ...)
{
panel.grid(h = -1, v = -1)
panel.abline(h = 0)
panel.xyplot(x, y, ...)
}
)
qqmath( ~ resid(alli.mod1),
xlab = "Theoretical Quantiles",
ylab = "Residuals"
)
rm(list=ls())
ls()
#Problema de Exemplo
#Paraesta analise utilizaremos o dataset "cars" nativo do R. cars ?? um dataset built-in padr??o do R que torna f??cil a demonstra????o de uma regress??o
#linear de modo simples e pr??tico.
#O dataset pode ser acessado diretamente apenas escrevendo cars no console do R. Ap??s isso ?? poss??vel identificar que o dataset consiste de 50 observa????es
#com duas vari??veis, sendo elas dist e speed, vamos verificar as primeiras cinco observa????es
head(cars)
#SCATTER PLOT
#scatter plot pode ajudar a visualizar qualquer rela????o linear entre preditores e desfechos. Idealmente se voc?? possui miltiplos preditores, o scatter plot ?? plotado para cada um deles
#contra a vari??vel de desfecho, atrav??s da linha de melhor ajuste, como visto abaixo.
par(mfrow=c(1,1))#divide a area de plotagem em uma coluna
scatter.smooth(x=cars$speed, y=cars$dist, mail="Dist ~Speed")#scatterplot
par(mfrow=c(1,2))#divide a area de plotagem em dias colunas
boxplot(cars$speed, main="Speed", sub=paste("Outlier rows: ", boxplot.stats(cars$speed)$out))  #plotagem para 'Speed'
boxplot(cars$dist, main="Distances", sub=paste("Outlier rows: ", boxplot.stats(cars$dist)$out))  #plotagem para 'Speed'
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir,
center = TRUE,
scale. = TRUE) 1
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE,scale. = TRUE)
# print method
print(ir.pca)
# plot method
plot(ir.pca, type = "l")
# summary method
summary(ir.pca)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
# print method
print(ir.pca)
# plot method
plot(ir.pca, type = "l")
# summary method
summary(ir.pca)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
library(devtools)
install_github("ggbiplot", "vqv")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
require(caret)
trans = preProcess(iris[,1:4],
method=c("BoxCox", "center",
"scale", "pca"))
PC = predict(trans, iris[,1:4])
# Retained PCs
head(PC, 3)
#https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE,scale. = TRUE)
# print method
print(ir.pca)
# plot method
plot(ir.pca, type = "l")
# plot method
plot(ir.pca, type = "l")
# summary method
summary(ir.pca)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
library(devtools)
install.packages("devtools")
install_github("ggbiplot", "vqv")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
install_github("ggbiplot", "vqv")
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
require(caret)
trans = preProcess(iris[,1:4],
method=c("BoxCox", "center",
"scale", "pca"))
PC = predict(trans, iris[,1:4])
# Retained PCs
head(PC, 3)
##EXEMPLO 2
data("iris")
str(iris)
summary(iris)
##partition data
set.seed(111)
ind <- sample(2. nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
##partition data
set.seed(111)
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
training <- iris[ind==1, ]
tresting <- iris[ind==2, ]
training
tresting
testing <- iris[ind==2, ]
##EXEMPLO 2
data("iris")
str(iris)
summary(iris)
##partition data
set.seed(111)
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
training <- iris[ind==1, ]
testing <- iris[ind==2, ]
##Scatter Plots e Coeficientes de Correlacao
library(psych)
##Scatter Plots e Coeficientes de Correlacao
library(psych)
pairs.panels(training[ ,-5], gap = 0, bg = c("red", "yellow", "blue")[training$Species])
##PCA principal component analysis
pc<- prcomp(trainingg[ ,-5], center = TRUE, scale. = TRUE)
##PCA principal component analysis
pc<- prcomp(training[ ,-5], center = TRUE, scale. = TRUE)
attributes(pc)
pc$center
mean(training$Sepal.Length)
pc$scale
sd(training$Sepal.Length)
pc
summary(pc)
##Ortoganalidade dos principais componentes
pairs.panels(pc$x,
gap=0,
bg=c("red","yellow","blue")[training$Species],
pch = 21)
##bi-plot
library(devtools)
##bi-plot
library(devtools)
library(ggbiplot)
g <- ggbiplot(pc, obs.scale = 1, var.scale = 1, groups = training$Species, ellipse = TRUE, circle = TRUE, ellipse.prob = 0.68)
g <- ggbiplot(pc, obs.scale = 1, var.scale = 1, groups = training$Species, ellipse = TRUE, circle = TRUE, ellipse.prob = 0.68)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
#https://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
# Load data
data(iris)
head(iris, 3)
# log transform
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
ir.pca <- prcomp(log.ir, center = TRUE,scale. = TRUE)
# print method
print(ir.pca)
# plot method
plot(ir.pca, type = "l")
# summary method
summary(ir.pca)
# Predict PCs
predict(ir.pca, newdata=tail(log.ir, 2))
install.packages("devtools")
install.packages("devtools")
library(ggbiplot)
g <- ggbiplot(ir.pca, obs.scale = 1, var.scale = 1,
groups = ir.species, ellipse = TRUE,
circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal',
legend.position = 'top')
print(g)
require(caret)
trans = preProcess(iris[,1:4],
method=c("BoxCox", "center",
"scale", "pca"))
PC = predict(trans, iris[,1:4])
# Retained PCs
head(PC, 3)
##EXEMPLO 2
data("iris")
str(iris)
summary(iris)
##partition data
set.seed(111)
ind <- sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2))
training <- iris[ind==1, ]
testing <- iris[ind==2, ]
##Scatter Plots e Coeficientes de Correlacao
library(psych)
pairs.panels(training[ ,-5], gap = 0, bg = c("red", "yellow", "blue")[training$Species])
##PCA principal component analysis
pc<- prcomp(training[ ,-5], center = TRUE, scale. = TRUE)
attributes(pc)
pc$center
mean(training$Sepal.Length)
pc$scale
sd(training$Sepal.Length)
pc
summary(pc)
##Ortoganalidade dos principais componentes
pairs.panels(pc$x,
gap=0,
bg=c("red","yellow","blue")[training$Species],
pch = 21)
##bi-plot
library(devtools)
library(ggbiplot)
g <- ggbiplot(pc, obs.scale = 1, var.scale = 1, groups = training$Species, ellipse = TRUE, circle = TRUE, ellipse.prob = 0.68)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', legend.position = 'top')
print(g)
library(swirl)
swirl()
str(diamonds)
qplot(proce, data = diamonds)
qplot(price, data = diamonds)
range(diamonds$price)
qplot(price, data = diamonds, binwidth = max(diamonds$price) / min(diamonds$price))
qplot(price, data = diamonds, binwidth = 18497/30)
brk
counts
qplot(price, data = diamonds, binwidth = 18497/30, fill = cut
qplot(price, data = diamonds, binwidth = 18497/30, fill = cut)
qplot(price, data = diamonds, geom = "desnsity")
qplot(price, data = diamonds, geom = "desnsity")
qplot(price, data = diamonds, geom = "density")
qplot(price, data = diamonds, geom = "density", color = cut)
qplot(carat, price, data = diamonds)
qplot(carat, price, data = diamonds, shape = cut)
qplot(carat, price, data = diamonds, color = cut)
qplot(carat, price, data = diamonds, color = cut, geom = "smooth")
qplot(carat, price, data = diamonds, color = cut) + geom_smooth(method = "lm")
qplot(carat, price, data = diamonds, color = cut, facets = .~cut()) + geom_smooth(method = "lm")
qplot(carat, price, data = diamonds, color = cut, facets = .~cut + geom_smooth(method = "lm")
qplot(carat, price, data = diamonds, color = cut, facets = .~cut) + geom_smooth(method = "lm")
qplot(carat, price, data = diamonds, color = cut) + geom_smooth(method = "lm", facets = .~cut)
qplot(carat, price, data = diamonds, color = cut, facets = .~cut) + geom_smooth(method = "lm")
g <- ggplot(data = diamonds, abs(depth, price))
g <- ggplot(data = diamonds, abs(depth, price))
g <- ggplot(data = diamonds, aes(depth, price))
summary(g)
g + geom_point(alpha = 1/3
g + geom_point(alpha = 1/3)
cutpoints <- quantile(diamonds$carat, seq(0,1, length = 4), na.rm = TRUE)
cutpoints
diamonds$car2 <- cut(diamonds$carat, cutpoints)
g <- ggplot(data = diamonds, aes(depth, price))
g + geom_point(alpha = 1/3) + facet_grid(cut ~ car2)
diamonds[myd,]
g + geom_point(method = "lm", size = 3, color = "pink") + facet_grid(cut ~ car2)
Type g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
ggplot(data = diamonds, aes(carat, price)) + geom_boxplot() = facet_grid(.~cut)
ggplot(data = diamonds, aes(carat, price)) + geom_boxplot() + facet_grid(.~cut)
setwd("../Exploratory Data Analysis/week4/coursera/eda/ex1/")
E
setwd("../Exploratory Data Analysis/week4/coursera/eda/ex1/")
setwd("../Exploratory Data Analysis/week4/coursera/eda/ex1/")
